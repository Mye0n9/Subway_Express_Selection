{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4482,"status":"ok","timestamp":1705600090135,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"d02O8aVrZfdx"},"outputs":[],"source":["import gensim\n","import nltk\n","from gensim import corpora\n","from gensim.models import LdaModel\n","from gensim.utils import simple_preprocess\n","from nltk.corpus import stopwords\n","\n","from langchain.chains import LLMChain\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.llms import OpenAI"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1705600090135,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"B6N4ANpCrxrU"},"outputs":[],"source":["from langchain.document_loaders.csv_loader import CSVLoader"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1434,"status":"ok","timestamp":1705600197164,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"d8H4Y1V_sfky","outputId":"adb83215-ef32-43c6-ab2a-5c32bf99f299"},"outputs":[{"data":{"text/plain":["Index(['Unnamed: 0', 'review'], dtype='object')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","pos_file = 'dataset/pos_reviews.csv'\n","neg_file = 'dataset/neg_reviews.csv'\n","df = pd.read_csv(pos_file)\n","df.columns"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":319,"status":"ok","timestamp":1705600210995,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"p-ZzcB3aTL0V"},"outputs":[],"source":["def preprocess(text, stop_words):\n","    \"\"\"\n","    Tokenizes and preprocesses the input text, removing stopwords and short\n","    tokens.\n","\n","    Parameters:\n","        text (str): The input text to preprocess.\n","        stop_words (set): A set of stopwords to be removed from the text.\n","    Returns:\n","        list: A list of preprocessed tokens.\n","    \"\"\"\n","    result = []\n","    for token in simple_preprocess(text, deacc=True):\n","        if token not in stop_words and len(token) > 3:\n","            result.append(token)\n","    return result"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1705600213237,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"bGOHOUr9TdzK"},"outputs":[],"source":["def get_topic_lists_from_csv(file, num_topics, words_per_topic):\n","    \"\"\"\n","    Extracts topics and their associated words from a PDF document using the\n","    Latent Dirichlet Allocation (LDA) algorithm.\n","\n","    Parameters:\n","        file (str): The path to the PDF file for topic extraction.\n","        num_topics (int): The number of topics to discover.\n","        words_per_topic (int): The number of words to include per topic.\n","\n","    Returns:\n","        list: A list of num_topics sublists, each containing relevant words\n","        for a topic.\n","    \"\"\"\n","    # Load the pdf file\n","    loader = CSVLoader(file_path= file, csv_args={\n","      'delimiter': ',',\n","      'quotechar': '\"',\n","      'fieldnames': ['Unnamed: 0', 'statnNm', 'rating', 'review', 'review_count']\n","    })\n","\n","    # Extract the text from each page into a list. Each page is considered a document\n","    documents= []\n","    for data in loader.load():\n","        documents.append(data.page_content)\n","\n","    # Preprocess the documents\n","    nltk.download('stopwords')\n","    stop_words = set(stopwords.words(['english','spanish']))\n","    processed_documents = [preprocess(doc, stop_words) for doc in documents]\n","\n","    # Create a dictionary and a corpus\n","    dictionary = corpora.Dictionary(processed_documents)\n","    corpus = [dictionary.doc2bow(doc) for doc in processed_documents]\n","\n","    # Build the LDA model\n","    lda_model = LdaModel(\n","        corpus,\n","        num_topics=num_topics,\n","        id2word=dictionary,\n","        passes=15\n","        )\n","\n","    # Retrieve the topics and their corresponding words\n","    topics = lda_model.print_topics(num_words=words_per_topic)\n","\n","    # Store each list of words from each topic into a list\n","    topics_ls = []\n","    for topic in topics:\n","        words = topic[1].split(\"+\")\n","        topic_words = [word.split(\"*\")[1].replace('\"', '').strip() for word in words]\n","        topics_ls.append(topic_words)\n","\n","    return topics_ls"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":312,"status":"ok","timestamp":1705600510324,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"kODBDypxtPxQ"},"outputs":[],"source":["pos_template = '''Describe the postive topic of each of the {num_topics}\n","        double-quote delimited lists in a simple phrase and also write down\n","        ten possible words that can describe the topic. The lists are the result of an\n","        algorithm for topic discovery. While selecting the topic, the pronouns are not allowed.\n","\n","        Do not provide an introduction or a conclusion, only describe the\n","        topics. Do not mention the word \"topic\" when describing the topics.\n","        Use the following template for the response.\n","\n","        1: <<<(sentence describing the topic)>>>\n","        - <<<(whether topic is positive or negative)>>>\n","        - <<<(words describing the topic)>>>\n","\n","        2: <<<(sentence describing the topic)>>>\n","        - <<<(whether topic is positive or negative)>>>\n","        - <<<(words describing the topic)>>>\n","\n","\n","        ...\n","\n","        n: <<<(sentence describing the topic)>>>\n","        - <<<(whether topic is positive or negative)>>>\n","        - <<<(words describing the topic)>>>\n","\n","\n","        Lists: \"\"\"{string_lda}\"\"\" '''\n","\n","neg_template = '''Describe the negative topics of each of the {num_topics}\n","        double-quote delimited lists in a simple phrase and also write down\n","        ten possible words that can describe the topic. The lists are the result of an\n","        algorithm for topic discovery. While selecting the topic, the pronouns are not allowed.\n","\n","        Do not provide an introduction or a conclusion, only describe the\n","        topics. Do not mention the word \"topic\" when describing the topics.\n","        Use the following template for the response.\n","\n","        1: <<<(sentence describing the topic)>>>\n","        - <<<(whether topic is positive or negative)>>>\n","        - <<<(words describing the topic)>>>\n","\n","        2: <<<(sentence describing the topic)>>>\n","        - <<<(whether topic is positive or negative)>>>\n","        - <<<(words describing the topic)>>>\n","\n","\n","        ...\n","\n","        n: <<<(sentence describing the topic)>>>\n","        - <<<(whether topic is positive or negative)>>>\n","        - <<<(words describing the topic)>>>\n","\n","\n","        Lists: \"\"\"{string_lda}\"\"\" '''"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1705600510992,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"igXlT12VTgy6"},"outputs":[],"source":["def topics_from_csv(llm, template, file, num_topics, words_per_topic):\n","    \"\"\"\n","    Generates descriptive prompts for LLM based on topic words extracted from a\n","    PDF document.\n","\n","    This function takes the output of `get_topic_lists_from_pdf` function,\n","    which consists of a list of topic-related words for each topic, and\n","    generates an output string in table of content format.\n","\n","    Parameters:\n","        llm (LLM): An instance of the Large Language Model (LLM) for generating\n","        responses.\n","        file (str): The path to the PDF file for extracting topic-related words.\n","        num_topics (int): The number of topics to consider.\n","        words_per_topic (int): The number of words per topic to include.\n","\n","    Returns:\n","        str: A response generated by the language model based on the provided\n","        topic words.\n","    \"\"\"\n","\n","    # Extract topics and convert to string\n","    list_of_topicwords = get_topic_lists_from_csv(file, num_topics,\n","                                                  words_per_topic)\n","    string_lda = \"\"\n","    for list in list_of_topicwords:\n","        string_lda += str(list) + \"\\n\"\n","\n","    # Create the template\n","    template_string = template\n","\n","    # LLM call\n","    print(string_lda)\n","\n","    prompt_template = ChatPromptTemplate.from_template(template_string)\n","    chain = LLMChain(llm=llm, prompt=prompt_template)\n","    response = chain.run({\n","        \"string_lda\" : string_lda,\n","        \"num_topics\" : num_topics\n","        })\n","\n","    return response"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":496,"status":"ok","timestamp":1705600512785,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"q0nzCta1TmGN"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/myeongseop.kim/miniconda3/envs/URP/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n","  warn_deprecated(\n"]}],"source":["openai_key = 'YOUR_API_KEY'\n","llm = OpenAI(openai_api_key=openai_key, max_tokens=-1)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57426,"status":"ok","timestamp":1705600577620,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"fcH271bETosB","outputId":"937d68c2-743a-461f-98bc-f6a6b7bf317b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/myeongseop.kim/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"name":"stdout","output_type":"stream","text":["['none', 'airport', 'station', 'review', 'unnamed', 'statnnm', 'rating', 'review_count', 'train', 'also', 'ticket', 'sinagawa', 'keikyu', 'people', 'haneda', 'express', 'stop', 'escalator', 'easy', 'elevator', 'good', 'gate', 'kawasaki', 'clean', 'want', 'entrance', 'line', 'narita', 'know', 'take']\n","['none', 'review', 'unnamed', 'statnnm', 'review_count', 'rating', 'station', 'good', 'line', 'also', 'like', 'think', 'time', 'want', 'place', 'clean', 'tokyo', 'easy', 'home', 'shibuya', 'yokohama', 'city', 'people', 'convenient', 'walk', 'transfer', 'enjoy', 'know', 'feel', 'seems']\n","['station', 'line', 'none', 'also', 'tokyo', 'many', 'transfer', 'shinjuku', 'convenient', 'time', 'review', 'review_count', 'statnnm', 'unnamed', 'rating', 'people', 'ticket', 'good', 'subway', 'think', 'home', 'exit', 'metro', 'gate', 'around', 'used', 'doei', 'walk', 'place', 'asakusa']\n","['station', 'none', 'line', 'good', 'review', 'rating', 'unnamed', 'statnnm', 'review_count', 'also', 'train', 'many', 'convenient', 'shopping', 'clean', 'place', 'shops', 'time', 'stop', 'express', 'around', 'front', 'think', 'like', 'subway', 'restaurants', 'store', 'park', 'easy', 'people']\n","['station', 'kyoto', 'people', 'osaka', 'kawara', 'hankyu', 'sijo', 'exit', 'takatsuki', 'sadang', 'shibuya', 'shopping', 'famous', 'place', 'like', 'sindorim', 'many', 'defense', 'umeda', 'temperature', 'karasuma', 'intersection', 'want', 'city', 'none', 'still', 'construction', 'underground', 'subway', 'without']\n","['good', 'subway', 'train', 'seoul', 'place', 'station', 'people', 'york', 'clean', 'like', 'always', 'beautiful', 'many', 'city', 'really', 'best', 'great', 'crowded', 'time', 'central', 'service', 'square', 'building', 'take', 'well', 'times', 'street', 'grand', 'visit', 'trains']\n","\n"]},{"name":"stderr","output_type":"stream","text":["/Users/myeongseop.kim/miniconda3/envs/URP/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]}],"source":["num_topics = 6\n","words_per_topic = 30\n","\n","summary = topics_from_csv(llm, pos_template, pos_file, num_topics, words_per_topic)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1705600577620,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"vT2MDKCoTsFl","outputId":"455b51be-4c3e-472d-a838-df294b46a643"},"outputs":[{"data":{"text/plain":["'\\n1: <<<The convenience of traveling to and from the airport>>>\\n- Positive\\n- Easy, convenient, clean, good, gate, entrance, line, stop, ticket, take\\n\\n2: <<<The overall experience of using an unnamed station>>>\\n- Neutral\\n- Review, review count, rating, also, think, time, place, clean, people, walk\\n\\n3: <<<The convenience of transferring at a busy station>>>\\n- Positive\\n- Convenient, transfer, time, review, review count, rating, people, good, exit, gate\\n\\n4: <<<A good experience at a popular and busy station>>>\\n- Positive\\n- Station, line, good, review, rating, convenient, shopping, clean, place, time\\n\\n5: <<<The famous and well-known stations in a specific city>>>\\n- Neutral\\n- Station, city, people, shopping, famous, place, like, many, temperature, intersection\\n\\n6: <<<The positive experience of using public transportation in a big city>>>\\n- Positive\\n- Good, subway, train, clean, people, city, great, service, well, crowded'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["summary"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27959,"status":"ok","timestamp":1705600606424,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"34iGA4WJVX4i","outputId":"e3c71f07-35a4-4d94-92a6-52a6bff4deaf"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/myeongseop.kim/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["['none', 'station', 'rating', 'statnnm', 'unnamed', 'review_count', 'review', 'line', 'good', 'ticket', 'also', 'people', 'time', 'think', 'gate', 'high', 'home', 'store', 'exit', 'difficult', 'infection', 'transfer', 'train', 'groove', 'side', 'stop', 'risk', 'many', 'area', 'walk']\n","['station', 'none', 'line', 'people', 'train', 'many', 'subway', 'review', 'rating', 'unnamed', 'review_count', 'statnnm', 'think', 'time', 'also', 'good', 'airport', 'elevator', 'express', 'high', 'restaurants', 'tokyo', 'entrance', 'speed', 'walk', 'around', 'convenience', 'shops', 'stores', 'minutes']\n","['none', 'station', 'review', 'unnamed', 'rating', 'review_count', 'statnnm', 'also', 'convenient', 'line', 'transfer', 'think', 'train', 'ticket', 'good', 'many', 'difficult', 'first', 'time', 'exit', 'near', 'express', 'home', 'make', 'restaurant', 'shops', 'stop', 'place', 'stairs', 'shop']\n","['station', 'none', 'line', 'review', 'unnamed', 'review_count', 'statnnm', 'rating', 'also', 'home', 'people', 'many', 'stop', 'exit', 'ticket', 'transfer', 'tokyo', 'elevator', 'subway', 'time', 'gate', 'think', 'know', 'used', 'want', 'groove', 'express', 'side', 'front', 'convenient']\n","['station', 'none', 'line', 'also', 'train', 'review_count', 'rating', 'statnnm', 'unnamed', 'review', 'time', 'people', 'many', 'around', 'good', 'tokyu', 'area', 'convenient', 'transfer', 'home', 'yokohama', 'want', 'stop', 'shops', 'like', 'walk', 'toyoko', 'subway', 'street', 'high']\n","['none', 'review', 'statnnm', 'unnamed', 'review_count', 'rating', 'station', 'line', 'stop', 'train', 'shinjuku', 'transfer', 'stops', 'difficult', 'express', 'convenient', 'ticket', 'home', 'time', 'subway', 'exit', 'many', 'gate', 'place', 'also', 'tokyo', 'minutes', 'people', 'city', 'used']\n","\n"]}],"source":["num_topics = 6\n","words_per_topic = 30\n","\n","summary = topics_from_csv(llm, neg_template, neg_file, num_topics, words_per_topic)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1705600606424,"user":{"displayName":"명섭김","userId":"13141532646470931710"},"user_tz":-540},"id":"2CP5GfbKs9vJ","outputId":"18433f56-e6e8-4014-9ee7-873bc9d90757"},"outputs":[{"data":{"text/plain":["'\\n1: <<<The station is difficult to navigate.>>>\\n- <<Negative>>\\n- <<Difficult, navigate, station, gate, transfer, exit, time, people, area, walk>>\\n\\n2: <<<The train is always crowded.>>>\\n- <<Negative>>\\n- <<Crowded, train, people, line, subway, transfer, time, station, ticket, shops>>\\n\\n3: <<<The station is not convenient.>>>\\n- <<Negative>>\\n- <<Not convenient, station, transfer, time, home, exit, stop, restaurant, shops, stairs>>\\n\\n4: <<<The station has confusing signs.>>>\\n- <<Negative>>\\n- <<Confusing, signs, station, transfer, time, entrance, people, line, gate, ticket>>\\n\\n5: <<<The train is often delayed.>>>\\n- <<Negative>>\\n- <<Delayed, train, time, transfer, station, express, people, shops, subway, line>>\\n\\n6: <<<The station is not well-maintained.>>>\\n- <<Negative>>\\n- <<Not well-maintained, station, transfer, ticket, time, exit, express, shops, subway, line>>'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8KC6Mdvs-Yp"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
